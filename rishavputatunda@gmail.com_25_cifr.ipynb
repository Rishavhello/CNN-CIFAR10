{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Copy of rishavputatunda@gmail.com_25_cifr.ipynb","provenance":[{"file_id":"16-fwEHK6Pl9j_Kt546WNb2wx9Z_ky6W-","timestamp":1565362286104}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"wVIx_KIigxPV","colab":{}},"source":["import keras\n","from keras.datasets import cifar10\n","from keras.models import Model, Sequential\n","from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n","from keras.layers import SeparableConv2D, MaxPooling2D, BatchNormalization, Conv2D, Conv1D\n","from keras.layers import Concatenate\n","from keras.optimizers import Adam\n","from keras import models, layers\n","from keras.models import Model\n","from keras.layers import BatchNormalization, Activation, Flatten, InputLayer, Lambda, add\n","from keras.optimizers import Adam\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.callbacks import ModelCheckpoint\n","from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping, LearningRateScheduler, CSVLogger"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UNHw6luQg3gc","outputId":"bdfcc15b-76b3-4acb-b7da-f8601bb09439","executionInfo":{"status":"ok","timestamp":1567666586010,"user_tz":-330,"elapsed":1263,"user":{"displayName":"Rishav Putatunda","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB8kwyOxYTSXj0jnrae5i0B54hb9hMZyLZRV1Ek=s64","userId":"02963166143804289584"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n","# backend\n","import warnings\n","warnings.filterwarnings('ignore')\n","import tensorflow as tf\n","# from tensorflow import keras\n","\n","# from keras import backend as k\n","\n","# Don't pre-allocate memory; allocate as-needed\n","'''import tensorflow as tf\n","config = tf.ConfigProto()\n","tf.config.gpu.set_per_process_memory_fraction(0.90)\n","tf.config.gpu.set_per_process_memory_growth(True)\n","config = tf.ConfigProto()\n","config.gpu_options.allow_growth = True\n","'''\n","# Create a session with the above options specified.\n","# k.tensorflow_backend.set_session(tf.Session(config=config))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'import tensorflow as tf\\nconfig = tf.ConfigProto()\\ntf.config.gpu.set_per_process_memory_fraction(0.90)\\ntf.config.gpu.set_per_process_memory_growth(True)\\nconfig = tf.ConfigProto()\\nconfig.gpu_options.allow_growth = True\\n'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dsO_yGxcg5D8","colab":{}},"source":["# Hyperparameters\n","batch_size = 64\n","num_classes = 10\n","epochs = 85\n","l = 6 \n","num_filter = 35 \n","compression = 1.0\n","dropout_rate = 0.20"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mB7o3zu1g6eT","outputId":"505bd96d-dfe9-4fa9-840a-a15faa3deb1f","executionInfo":{"status":"ok","timestamp":1567666597404,"user_tz":-330,"elapsed":7203,"user":{"displayName":"Rishav Putatunda","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB8kwyOxYTSXj0jnrae5i0B54hb9hMZyLZRV1Ek=s64","userId":"02963166143804289584"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# Load CIFAR10 Data\n","from sklearn.model_selection import train_test_split\n","(temp, y_temp), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n","X_train, X_cv, y_train, y_cv = train_test_split(temp, y_temp, random_state=41, test_size=0.1)\n","\n","img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n","\n","# convert to one hot encoing \n","y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n","y_test = tf.keras.utils.to_categorical(y_test, num_classes) \n","y_cv = tf.keras.utils.to_categorical(y_cv, num_classes)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170500096/170498071 [==============================] - 2s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XLtHxyuBt_E7","colab_type":"code","outputId":"b71cd9da-81c8-4580-f25d-1303e4b2f5d0","executionInfo":{"status":"ok","timestamp":1567666598243,"user_tz":-330,"elapsed":6306,"user":{"displayName":"Rishav Putatunda","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB8kwyOxYTSXj0jnrae5i0B54hb9hMZyLZRV1Ek=s64","userId":"02963166143804289584"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["import numpy as np\n","X_train = X_train.astype('float64')\n","X_cv = X_cv.astype('float64')\n","X_test = X_test.astype('float64')\n","\n","X_train /= 255\n","X_cv /= 255\n","X_test /= 255\n","\n","print(X_train.shape)\n","print(X_cv.shape)\n","print(X_test.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(45000, 32, 32, 3)\n","(5000, 32, 32, 3)\n","(10000, 32, 32, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7ZX10ZpXMGyW","colab":{}},"source":["def denseblock(input, num_filter = 12, dropout_rate = 0.2):\n","    global compression\n","    temp = input\n","    for _ in range(l):\n","        BatchNorm = BatchNormalization()(temp)\n","        relu = Activation('relu')(BatchNorm)\n","        Conv2D_3_3 = Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n","        if dropout_rate>0:\n","            Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n","        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n","        \n","        temp = concat\n","        \n","    return temp\n","\n","def transition(input, num_filter = 12, dropout_rate = 0.2):\n","    global compression\n","    BatchNorm = BatchNormalization()(input)\n","    relu = Activation('relu')(BatchNorm)\n","    Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n","    if dropout_rate>0:\n","        Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n","    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n","    \n","    return avg\n","\n","def output_layer(input):\n","    global compression\n","    BatchNorm = BatchNormalization()(input)\n","    relu = Activation('relu')(BatchNorm)\n","    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n","    #flat = Flatten()(AvgPooling)\n","    output = Flatten()(Conv2D(num_classes, 2, activation='softmax')(AvgPooling))\n","    \n","    return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y-bGJmhfKjUd","colab_type":"code","colab":{}},"source":["input = Input(shape=(img_height, img_width, channel,))\n","First_Conv2D = Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n","\n","First_Block = denseblock(First_Conv2D, num_filter, dropout_rate)\n","First_Transition = transition(First_Block, num_filter, dropout_rate)\n","\n","Second_Block = denseblock(First_Transition, num_filter, dropout_rate)\n","Second_Transition = transition(Second_Block, num_filter, dropout_rate)\n","\n","Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n","Third_Transition = transition(Third_Block, num_filter, dropout_rate)\n","\n","Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n","output = output_layer(Last_Block)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1kFh7pdxhNtT","outputId":"eaebb097-d0a3-434e-e0b0-189fafa3e9f0","scrolled":true,"executionInfo":{"status":"ok","timestamp":1567666928587,"user_tz":-330,"elapsed":1446,"user":{"displayName":"Rishav Putatunda","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB8kwyOxYTSXj0jnrae5i0B54hb9hMZyLZRV1Ek=s64","userId":"02963166143804289584"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model = Model(inputs=[input], outputs=[output])\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_6 (InputLayer)            (None, 32, 32, 3)    0                                            \n","__________________________________________________________________________________________________\n","conv2d_142 (Conv2D)             (None, 32, 32, 35)   945         input_6[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_141 (BatchN (None, 32, 32, 35)   140         conv2d_142[0][0]                 \n","__________________________________________________________________________________________________\n","activation_141 (Activation)     (None, 32, 32, 35)   0           batch_normalization_141[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_143 (Conv2D)             (None, 32, 32, 35)   11025       activation_141[0][0]             \n","__________________________________________________________________________________________________\n","dropout_136 (Dropout)           (None, 32, 32, 35)   0           conv2d_143[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_121 (Concatenate)   (None, 32, 32, 70)   0           conv2d_142[0][0]                 \n","                                                                 dropout_136[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization_142 (BatchN (None, 32, 32, 70)   280         concatenate_121[0][0]            \n","__________________________________________________________________________________________________\n","activation_142 (Activation)     (None, 32, 32, 70)   0           batch_normalization_142[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_144 (Conv2D)             (None, 32, 32, 35)   22050       activation_142[0][0]             \n","__________________________________________________________________________________________________\n","dropout_137 (Dropout)           (None, 32, 32, 35)   0           conv2d_144[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_122 (Concatenate)   (None, 32, 32, 105)  0           concatenate_121[0][0]            \n","                                                                 dropout_137[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization_143 (BatchN (None, 32, 32, 105)  420         concatenate_122[0][0]            \n","__________________________________________________________________________________________________\n","activation_143 (Activation)     (None, 32, 32, 105)  0           batch_normalization_143[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_145 (Conv2D)             (None, 32, 32, 35)   33075       activation_143[0][0]             \n","__________________________________________________________________________________________________\n","dropout_138 (Dropout)           (None, 32, 32, 35)   0           conv2d_145[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_123 (Concatenate)   (None, 32, 32, 140)  0           concatenate_122[0][0]            \n","                                                                 dropout_138[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization_144 (BatchN (None, 32, 32, 140)  560         concatenate_123[0][0]            \n","__________________________________________________________________________________________________\n","activation_144 (Activation)     (None, 32, 32, 140)  0           batch_normalization_144[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_146 (Conv2D)             (None, 32, 32, 35)   44100       activation_144[0][0]             \n","__________________________________________________________________________________________________\n","dropout_139 (Dropout)           (None, 32, 32, 35)   0           conv2d_146[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_124 (Concatenate)   (None, 32, 32, 175)  0           concatenate_123[0][0]            \n","                                                                 dropout_139[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization_145 (BatchN (None, 32, 32, 175)  700         concatenate_124[0][0]            \n","__________________________________________________________________________________________________\n","activation_145 (Activation)     (None, 32, 32, 175)  0           batch_normalization_145[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_147 (Conv2D)             (None, 32, 32, 35)   55125       activation_145[0][0]             \n","__________________________________________________________________________________________________\n","dropout_140 (Dropout)           (None, 32, 32, 35)   0           conv2d_147[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_125 (Concatenate)   (None, 32, 32, 210)  0           concatenate_124[0][0]            \n","                                                                 dropout_140[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization_146 (BatchN (None, 32, 32, 210)  840         concatenate_125[0][0]            \n","__________________________________________________________________________________________________\n","activation_146 (Activation)     (None, 32, 32, 210)  0           batch_normalization_146[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_148 (Conv2D)             (None, 32, 32, 35)   66150       activation_146[0][0]             \n","__________________________________________________________________________________________________\n","dropout_141 (Dropout)           (None, 32, 32, 35)   0           conv2d_148[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_126 (Concatenate)   (None, 32, 32, 245)  0           concatenate_125[0][0]            \n","                                                                 dropout_141[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization_147 (BatchN (None, 32, 32, 245)  980         concatenate_126[0][0]            \n","__________________________________________________________________________________________________\n","activation_147 (Activation)     (None, 32, 32, 245)  0           batch_normalization_147[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_149 (Conv2D)             (None, 32, 32, 35)   8575        activation_147[0][0]             \n","__________________________________________________________________________________________________\n","dropout_142 (Dropout)           (None, 32, 32, 35)   0           conv2d_149[0][0]                 \n","__________________________________________________________________________________________________\n","average_pooling2d_21 (AveragePo (None, 16, 16, 35)   0           dropout_142[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization_148 (BatchN (None, 16, 16, 35)   140         average_pooling2d_21[0][0]       \n","__________________________________________________________________________________________________\n","activation_148 (Activation)     (None, 16, 16, 35)   0           batch_normalization_148[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_150 (Conv2D)             (None, 16, 16, 35)   11025       activation_148[0][0]             \n","__________________________________________________________________________________________________\n","dropout_143 (Dropout)           (None, 16, 16, 35)   0           conv2d_150[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_127 (Concatenate)   (None, 16, 16, 70)   0           average_pooling2d_21[0][0]       \n","                                                                 dropout_143[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization_149 (BatchN (None, 16, 16, 70)   280         concatenate_127[0][0]            \n","__________________________________________________________________________________________________\n","activation_149 (Activation)     (None, 16, 16, 70)   0           batch_normalization_149[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_151 (Conv2D)             (None, 16, 16, 35)   22050       activation_149[0][0]             \n","__________________________________________________________________________________________________\n","dropout_144 (Dropout)           (None, 16, 16, 35)   0           conv2d_151[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_128 (Concatenate)   (None, 16, 16, 105)  0           concatenate_127[0][0]            \n","                                                                 dropout_144[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization_150 (BatchN (None, 16, 16, 105)  420         concatenate_128[0][0]            \n","__________________________________________________________________________________________________\n","activation_150 (Activation)     (None, 16, 16, 105)  0           batch_normalization_150[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_152 (Conv2D)             (None, 16, 16, 35)   33075       activation_150[0][0]             \n","__________________________________________________________________________________________________\n","dropout_145 (Dropout)           (None, 16, 16, 35)   0           conv2d_152[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_129 (Concatenate)   (None, 16, 16, 140)  0           concatenate_128[0][0]            \n","                                                                 dropout_145[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization_151 (BatchN (None, 16, 16, 140)  560         concatenate_129[0][0]            \n","__________________________________________________________________________________________________\n","activation_151 (Activation)     (None, 16, 16, 140)  0           batch_normalization_151[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_153 (Conv2D)             (None, 16, 16, 35)   44100       activation_151[0][0]             \n","__________________________________________________________________________________________________\n","dropout_146 (Dropout)           (None, 16, 16, 35)   0           conv2d_153[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_130 (Concatenate)   (None, 16, 16, 175)  0           concatenate_129[0][0]            \n","                                                                 dropout_146[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization_152 (BatchN (None, 16, 16, 175)  700         concatenate_130[0][0]            \n","__________________________________________________________________________________________________\n","activation_152 (Activation)     (None, 16, 16, 175)  0           batch_normalization_152[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_154 (Conv2D)             (None, 16, 16, 35)   55125       activation_152[0][0]             \n","__________________________________________________________________________________________________\n","dropout_147 (Dropout)           (None, 16, 16, 35)   0           conv2d_154[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_131 (Concatenate)   (None, 16, 16, 210)  0           concatenate_130[0][0]            \n","                                                                 dropout_147[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization_153 (BatchN (None, 16, 16, 210)  840         concatenate_131[0][0]            \n","__________________________________________________________________________________________________\n","activation_153 (Activation)     (None, 16, 16, 210)  0           batch_normalization_153[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_155 (Conv2D)             (None, 16, 16, 35)   66150       activation_153[0][0]             \n","__________________________________________________________________________________________________\n","dropout_148 (Dropout)           (None, 16, 16, 35)   0           conv2d_155[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_132 (Concatenate)   (None, 16, 16, 245)  0           concatenate_131[0][0]            \n","                                                                 dropout_148[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization_154 (BatchN (None, 16, 16, 245)  980         concatenate_132[0][0]            \n","__________________________________________________________________________________________________\n","activation_154 (Activation)     (None, 16, 16, 245)  0           batch_normalization_154[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_156 (Conv2D)             (None, 16, 16, 35)   8575        activation_154[0][0]             \n","__________________________________________________________________________________________________\n","dropout_149 (Dropout)           (None, 16, 16, 35)   0           conv2d_156[0][0]                 \n","__________________________________________________________________________________________________\n","average_pooling2d_22 (AveragePo (None, 8, 8, 35)     0           dropout_149[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization_155 (BatchN (None, 8, 8, 35)     140         average_pooling2d_22[0][0]       \n","__________________________________________________________________________________________________\n","activation_155 (Activation)     (None, 8, 8, 35)     0           batch_normalization_155[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_157 (Conv2D)             (None, 8, 8, 35)     11025       activation_155[0][0]             \n","__________________________________________________________________________________________________\n","dropout_150 (Dropout)           (None, 8, 8, 35)     0           conv2d_157[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_133 (Concatenate)   (None, 8, 8, 70)     0           average_pooling2d_22[0][0]       \n","                                                                 dropout_150[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization_156 (BatchN (None, 8, 8, 70)     280         concatenate_133[0][0]            \n","__________________________________________________________________________________________________\n","activation_156 (Activation)     (None, 8, 8, 70)     0           batch_normalization_156[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_158 (Conv2D)             (None, 8, 8, 35)     22050       activation_156[0][0]             \n","__________________________________________________________________________________________________\n","dropout_151 (Dropout)           (None, 8, 8, 35)     0           conv2d_158[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_134 (Concatenate)   (None, 8, 8, 105)    0           concatenate_133[0][0]            \n","                                                                 dropout_151[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization_157 (BatchN (None, 8, 8, 105)    420         concatenate_134[0][0]            \n","__________________________________________________________________________________________________\n","activation_157 (Activation)     (None, 8, 8, 105)    0           batch_normalization_157[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_159 (Conv2D)             (None, 8, 8, 35)     33075       activation_157[0][0]             \n","__________________________________________________________________________________________________\n","dropout_152 (Dropout)           (None, 8, 8, 35)     0           conv2d_159[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_135 (Concatenate)   (None, 8, 8, 140)    0           concatenate_134[0][0]            \n","                                                                 dropout_152[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization_158 (BatchN (None, 8, 8, 140)    560         concatenate_135[0][0]            \n","__________________________________________________________________________________________________\n","activation_158 (Activation)     (None, 8, 8, 140)    0           batch_normalization_158[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_160 (Conv2D)             (None, 8, 8, 35)     44100       activation_158[0][0]             \n","__________________________________________________________________________________________________\n","dropout_153 (Dropout)           (None, 8, 8, 35)     0           conv2d_160[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_136 (Concatenate)   (None, 8, 8, 175)    0           concatenate_135[0][0]            \n","                                                                 dropout_153[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization_159 (BatchN (None, 8, 8, 175)    700         concatenate_136[0][0]            \n","__________________________________________________________________________________________________\n","activation_159 (Activation)     (None, 8, 8, 175)    0           batch_normalization_159[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_161 (Conv2D)             (None, 8, 8, 35)     55125       activation_159[0][0]             \n","__________________________________________________________________________________________________\n","dropout_154 (Dropout)           (None, 8, 8, 35)     0           conv2d_161[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_137 (Concatenate)   (None, 8, 8, 210)    0           concatenate_136[0][0]            \n","                                                                 dropout_154[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization_160 (BatchN (None, 8, 8, 210)    840         concatenate_137[0][0]            \n","__________________________________________________________________________________________________\n","activation_160 (Activation)     (None, 8, 8, 210)    0           batch_normalization_160[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_162 (Conv2D)             (None, 8, 8, 35)     66150       activation_160[0][0]             \n","__________________________________________________________________________________________________\n","dropout_155 (Dropout)           (None, 8, 8, 35)     0           conv2d_162[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_138 (Concatenate)   (None, 8, 8, 245)    0           concatenate_137[0][0]            \n","                                                                 dropout_155[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization_161 (BatchN (None, 8, 8, 245)    980         concatenate_138[0][0]            \n","__________________________________________________________________________________________________\n","activation_161 (Activation)     (None, 8, 8, 245)    0           batch_normalization_161[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_163 (Conv2D)             (None, 8, 8, 35)     8575        activation_161[0][0]             \n","__________________________________________________________________________________________________\n","dropout_156 (Dropout)           (None, 8, 8, 35)     0           conv2d_163[0][0]                 \n","__________________________________________________________________________________________________\n","average_pooling2d_23 (AveragePo (None, 4, 4, 35)     0           dropout_156[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization_162 (BatchN (None, 4, 4, 35)     140         average_pooling2d_23[0][0]       \n","__________________________________________________________________________________________________\n","activation_162 (Activation)     (None, 4, 4, 35)     0           batch_normalization_162[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_164 (Conv2D)             (None, 4, 4, 35)     11025       activation_162[0][0]             \n","__________________________________________________________________________________________________\n","dropout_157 (Dropout)           (None, 4, 4, 35)     0           conv2d_164[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_139 (Concatenate)   (None, 4, 4, 70)     0           average_pooling2d_23[0][0]       \n","                                                                 dropout_157[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization_163 (BatchN (None, 4, 4, 70)     280         concatenate_139[0][0]            \n","__________________________________________________________________________________________________\n","activation_163 (Activation)     (None, 4, 4, 70)     0           batch_normalization_163[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_165 (Conv2D)             (None, 4, 4, 35)     22050       activation_163[0][0]             \n","__________________________________________________________________________________________________\n","dropout_158 (Dropout)           (None, 4, 4, 35)     0           conv2d_165[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_140 (Concatenate)   (None, 4, 4, 105)    0           concatenate_139[0][0]            \n","                                                                 dropout_158[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization_164 (BatchN (None, 4, 4, 105)    420         concatenate_140[0][0]            \n","__________________________________________________________________________________________________\n","activation_164 (Activation)     (None, 4, 4, 105)    0           batch_normalization_164[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_166 (Conv2D)             (None, 4, 4, 35)     33075       activation_164[0][0]             \n","__________________________________________________________________________________________________\n","dropout_159 (Dropout)           (None, 4, 4, 35)     0           conv2d_166[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_141 (Concatenate)   (None, 4, 4, 140)    0           concatenate_140[0][0]            \n","                                                                 dropout_159[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization_165 (BatchN (None, 4, 4, 140)    560         concatenate_141[0][0]            \n","__________________________________________________________________________________________________\n","activation_165 (Activation)     (None, 4, 4, 140)    0           batch_normalization_165[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_167 (Conv2D)             (None, 4, 4, 35)     44100       activation_165[0][0]             \n","__________________________________________________________________________________________________\n","dropout_160 (Dropout)           (None, 4, 4, 35)     0           conv2d_167[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_142 (Concatenate)   (None, 4, 4, 175)    0           concatenate_141[0][0]            \n","                                                                 dropout_160[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization_166 (BatchN (None, 4, 4, 175)    700         concatenate_142[0][0]            \n","__________________________________________________________________________________________________\n","activation_166 (Activation)     (None, 4, 4, 175)    0           batch_normalization_166[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_168 (Conv2D)             (None, 4, 4, 35)     55125       activation_166[0][0]             \n","__________________________________________________________________________________________________\n","dropout_161 (Dropout)           (None, 4, 4, 35)     0           conv2d_168[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_143 (Concatenate)   (None, 4, 4, 210)    0           concatenate_142[0][0]            \n","                                                                 dropout_161[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization_167 (BatchN (None, 4, 4, 210)    840         concatenate_143[0][0]            \n","__________________________________________________________________________________________________\n","activation_167 (Activation)     (None, 4, 4, 210)    0           batch_normalization_167[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_169 (Conv2D)             (None, 4, 4, 35)     66150       activation_167[0][0]             \n","__________________________________________________________________________________________________\n","dropout_162 (Dropout)           (None, 4, 4, 35)     0           conv2d_169[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_144 (Concatenate)   (None, 4, 4, 245)    0           concatenate_143[0][0]            \n","                                                                 dropout_162[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization_168 (BatchN (None, 4, 4, 245)    980         concatenate_144[0][0]            \n","__________________________________________________________________________________________________\n","activation_168 (Activation)     (None, 4, 4, 245)    0           batch_normalization_168[0][0]    \n","__________________________________________________________________________________________________\n","average_pooling2d_24 (AveragePo (None, 2, 2, 245)    0           activation_168[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_170 (Conv2D)             (None, 1, 1, 10)     9810        average_pooling2d_24[0][0]       \n","__________________________________________________________________________________________________\n","flatten_9 (Flatten)             (None, 10)           0           conv2d_170[0][0]                 \n","==================================================================================================\n","Total params: 978,260\n","Trainable params: 970,420\n","Non-trainable params: 7,840\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"b4XOsW3ahSkL","outputId":"38f0b3b3-bd0e-4735-fa2f-7967328e5d1c","executionInfo":{"status":"ok","timestamp":1567666933707,"user_tz":-330,"elapsed":1047,"user":{"displayName":"Rishav Putatunda","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB8kwyOxYTSXj0jnrae5i0B54hb9hMZyLZRV1Ek=s64","userId":"02963166143804289584"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["# determine Loss function and Optimizer\n","\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer='Adam',\n","              metrics=['accuracy'])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["W0905 07:02:16.867691 139990985037696 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"KkzGMIYULUHw","colab_type":"code","colab":{}},"source":["\n","\n","\n","def decay_fn(epoch, lr):\n","    if epoch < 50:\n","        return 0.001\n","    elif epoch >= 50 and epoch < 75:\n","        return 0.0001\n","    else:\n","        return 0.00001\n","\n","lr_scheduler = keras.callbacks.LearningRateScheduler(decay_fn)\n","\n","filepath = \"{epoch:03d}-{val_acc:.3f}.hdf5\"\n","model_chkpt = ModelCheckpoint(filepath, monitor = \"val_loss\", save_best_only=True, verbose = 1)\n","\n","call = [lr_scheduler, model_chkpt]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lw1TG2powAOt","colab_type":"code","colab":{}},"source":["model = keras.models.load_model('/content/MODEL5-weights.31-0.96.hdf5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5kfn_zm-JedI","colab_type":"code","colab":{}},"source":["####### NEW #############\n","datagen_train = ImageDataGenerator(\n","    rotation_range=20,\n","    width_shift_range=0.125,\n","    height_shift_range=0.125,\n","    horizontal_flip=True,\n","    fill_mode='nearest',\n","    zoom_range=0.10\n",")\n","\n","datagen_train.fit(X_train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"982aaaaa-896e-4018-d4b7-8ec2c12dcf2e","executionInfo":{"status":"ok","timestamp":1567677136642,"user_tz":-330,"elapsed":10148175,"user":{"displayName":"Rishav Putatunda","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB8kwyOxYTSXj0jnrae5i0B54hb9hMZyLZRV1Ek=s64","userId":"02963166143804289584"}},"id":"Ac_GsxfzNj3v","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# fits the model on batches with real-time data augmentation:\n","model.fit_generator(datagen_train.flow(X_train, y_train, batch_size=batch_size),\n","                    steps_per_epoch=len(X_train)//batch_size, epochs=55, initial_epoch=0, verbose=1, validation_data=(X_cv, y_cv), callbacks=call)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/55\n","703/703 [==============================] - 203s 289ms/step - loss: 1.5496 - acc: 0.4320 - val_loss: 1.7978 - val_acc: 0.4870\n","\n","Epoch 00001: val_loss improved from inf to 1.79780, saving model to 001-0.487.hdf5\n","Epoch 2/55\n","703/703 [==============================] - 183s 261ms/step - loss: 1.1724 - acc: 0.5800 - val_loss: 1.3119 - val_acc: 0.5710\n","\n","Epoch 00002: val_loss improved from 1.79780 to 1.31185, saving model to 002-0.571.hdf5\n","Epoch 3/55\n","703/703 [==============================] - 183s 261ms/step - loss: 0.9937 - acc: 0.6449 - val_loss: 2.1834 - val_acc: 0.4856\n","\n","Epoch 00003: val_loss did not improve from 1.31185\n","Epoch 4/55\n","703/703 [==============================] - 183s 261ms/step - loss: 0.8854 - acc: 0.6842 - val_loss: 1.1471 - val_acc: 0.6600\n","\n","Epoch 00004: val_loss improved from 1.31185 to 1.14705, saving model to 004-0.660.hdf5\n","Epoch 5/55\n","703/703 [==============================] - 183s 260ms/step - loss: 0.8072 - acc: 0.7153 - val_loss: 1.1943 - val_acc: 0.6606\n","\n","Epoch 00005: val_loss did not improve from 1.14705\n","Epoch 6/55\n","703/703 [==============================] - 184s 262ms/step - loss: 0.7530 - acc: 0.7354 - val_loss: 0.8528 - val_acc: 0.7332\n","\n","Epoch 00006: val_loss improved from 1.14705 to 0.85276, saving model to 006-0.733.hdf5\n","Epoch 7/55\n","703/703 [==============================] - 184s 262ms/step - loss: 0.6975 - acc: 0.7576 - val_loss: 1.2876 - val_acc: 0.6600\n","\n","Epoch 00007: val_loss did not improve from 0.85276\n","Epoch 8/55\n","703/703 [==============================] - 184s 262ms/step - loss: 0.6660 - acc: 0.7654 - val_loss: 0.9025 - val_acc: 0.7206\n","\n","Epoch 00008: val_loss did not improve from 0.85276\n","Epoch 9/55\n","703/703 [==============================] - 184s 262ms/step - loss: 0.6375 - acc: 0.7773 - val_loss: 1.3793 - val_acc: 0.6524\n","\n","Epoch 00009: val_loss did not improve from 0.85276\n","Epoch 10/55\n","703/703 [==============================] - 184s 262ms/step - loss: 0.6111 - acc: 0.7853 - val_loss: 0.7724 - val_acc: 0.7652\n","\n","Epoch 00010: val_loss improved from 0.85276 to 0.77239, saving model to 010-0.765.hdf5\n","Epoch 11/55\n","703/703 [==============================] - 184s 261ms/step - loss: 0.5821 - acc: 0.7970 - val_loss: 0.8013 - val_acc: 0.7596\n","\n","Epoch 00011: val_loss did not improve from 0.77239\n","Epoch 12/55\n","703/703 [==============================] - 184s 261ms/step - loss: 0.5704 - acc: 0.8010 - val_loss: 1.0041 - val_acc: 0.7330\n","\n","Epoch 00012: val_loss did not improve from 0.77239\n","Epoch 13/55\n","703/703 [==============================] - 184s 262ms/step - loss: 0.5407 - acc: 0.8131 - val_loss: 0.7374 - val_acc: 0.7782\n","\n","Epoch 00013: val_loss improved from 0.77239 to 0.73736, saving model to 013-0.778.hdf5\n","Epoch 14/55\n","703/703 [==============================] - 184s 262ms/step - loss: 0.5230 - acc: 0.8187 - val_loss: 0.5828 - val_acc: 0.8132\n","\n","Epoch 00014: val_loss improved from 0.73736 to 0.58283, saving model to 014-0.813.hdf5\n","Epoch 15/55\n","703/703 [==============================] - 184s 262ms/step - loss: 0.5111 - acc: 0.8220 - val_loss: 0.6701 - val_acc: 0.8012\n","\n","Epoch 00015: val_loss did not improve from 0.58283\n","Epoch 16/55\n","703/703 [==============================] - 184s 262ms/step - loss: 0.4896 - acc: 0.8292 - val_loss: 0.5474 - val_acc: 0.8254\n","\n","Epoch 00016: val_loss improved from 0.58283 to 0.54742, saving model to 016-0.825.hdf5\n","Epoch 17/55\n","703/703 [==============================] - 184s 262ms/step - loss: 0.4804 - acc: 0.8336 - val_loss: 0.6197 - val_acc: 0.8084\n","\n","Epoch 00017: val_loss did not improve from 0.54742\n","Epoch 18/55\n","703/703 [==============================] - 184s 262ms/step - loss: 0.4749 - acc: 0.8360 - val_loss: 0.6708 - val_acc: 0.8050\n","\n","Epoch 00018: val_loss did not improve from 0.54742\n","Epoch 19/55\n","703/703 [==============================] - 184s 262ms/step - loss: 0.4592 - acc: 0.8401 - val_loss: 0.6554 - val_acc: 0.8096\n","\n","Epoch 00019: val_loss did not improve from 0.54742\n","Epoch 20/55\n","703/703 [==============================] - 184s 262ms/step - loss: 0.4474 - acc: 0.8421 - val_loss: 0.5307 - val_acc: 0.8290\n","\n","Epoch 00020: val_loss improved from 0.54742 to 0.53074, saving model to 020-0.829.hdf5\n","Epoch 21/55\n","703/703 [==============================] - 184s 261ms/step - loss: 0.4373 - acc: 0.8484 - val_loss: 0.4502 - val_acc: 0.8534\n","\n","Epoch 00021: val_loss improved from 0.53074 to 0.45020, saving model to 021-0.853.hdf5\n","Epoch 22/55\n","703/703 [==============================] - 184s 262ms/step - loss: 0.4258 - acc: 0.8523 - val_loss: 0.5846 - val_acc: 0.8328\n","\n","Epoch 00022: val_loss did not improve from 0.45020\n","Epoch 23/55\n","703/703 [==============================] - 183s 261ms/step - loss: 0.4222 - acc: 0.8528 - val_loss: 0.5261 - val_acc: 0.8428\n","\n","Epoch 00023: val_loss did not improve from 0.45020\n","Epoch 24/55\n","703/703 [==============================] - 183s 261ms/step - loss: 0.4097 - acc: 0.8578 - val_loss: 0.5177 - val_acc: 0.8416\n","\n","Epoch 00024: val_loss did not improve from 0.45020\n","Epoch 25/55\n","703/703 [==============================] - 183s 261ms/step - loss: 0.4019 - acc: 0.8586 - val_loss: 0.5554 - val_acc: 0.8214\n","\n","Epoch 00025: val_loss did not improve from 0.45020\n","Epoch 26/55\n","703/703 [==============================] - 183s 261ms/step - loss: 0.3961 - acc: 0.8620 - val_loss: 0.4740 - val_acc: 0.8526\n","\n","Epoch 00026: val_loss did not improve from 0.45020\n","Epoch 27/55\n","703/703 [==============================] - 184s 262ms/step - loss: 0.3840 - acc: 0.8666 - val_loss: 0.5091 - val_acc: 0.8442\n","\n","Epoch 00027: val_loss did not improve from 0.45020\n","Epoch 28/55\n","703/703 [==============================] - 184s 261ms/step - loss: 0.3770 - acc: 0.8683 - val_loss: 0.3568 - val_acc: 0.8838\n","\n","Epoch 00028: val_loss improved from 0.45020 to 0.35683, saving model to 028-0.884.hdf5\n","Epoch 29/55\n","703/703 [==============================] - 184s 261ms/step - loss: 0.3700 - acc: 0.8699 - val_loss: 0.4298 - val_acc: 0.8662\n","\n","Epoch 00029: val_loss did not improve from 0.35683\n","Epoch 30/55\n","703/703 [==============================] - 184s 261ms/step - loss: 0.3703 - acc: 0.8704 - val_loss: 0.4877 - val_acc: 0.8548\n","\n","Epoch 00030: val_loss did not improve from 0.35683\n","Epoch 31/55\n","703/703 [==============================] - 184s 261ms/step - loss: 0.3568 - acc: 0.8759 - val_loss: 0.4538 - val_acc: 0.8624\n","\n","Epoch 00031: val_loss did not improve from 0.35683\n","Epoch 32/55\n","703/703 [==============================] - 184s 261ms/step - loss: 0.3561 - acc: 0.8748 - val_loss: 0.4719 - val_acc: 0.8582\n","\n","Epoch 00032: val_loss did not improve from 0.35683\n","Epoch 33/55\n","703/703 [==============================] - 184s 261ms/step - loss: 0.3464 - acc: 0.8781 - val_loss: 0.4655 - val_acc: 0.8628\n","\n","Epoch 00033: val_loss did not improve from 0.35683\n","Epoch 34/55\n","703/703 [==============================] - 184s 261ms/step - loss: 0.3428 - acc: 0.8807 - val_loss: 0.4849 - val_acc: 0.8542\n","\n","Epoch 00034: val_loss did not improve from 0.35683\n","Epoch 35/55\n","703/703 [==============================] - 183s 261ms/step - loss: 0.3372 - acc: 0.8830 - val_loss: 0.3951 - val_acc: 0.8828\n","\n","Epoch 00035: val_loss did not improve from 0.35683\n","Epoch 36/55\n","703/703 [==============================] - 183s 261ms/step - loss: 0.3330 - acc: 0.8838 - val_loss: 0.6981 - val_acc: 0.8072\n","\n","Epoch 00036: val_loss did not improve from 0.35683\n","Epoch 37/55\n","703/703 [==============================] - 184s 261ms/step - loss: 0.3332 - acc: 0.8836 - val_loss: 0.3686 - val_acc: 0.8846\n","\n","Epoch 00037: val_loss did not improve from 0.35683\n","Epoch 38/55\n","703/703 [==============================] - 184s 261ms/step - loss: 0.3261 - acc: 0.8882 - val_loss: 0.4382 - val_acc: 0.8688\n","\n","Epoch 00038: val_loss did not improve from 0.35683\n","Epoch 39/55\n","703/703 [==============================] - 183s 261ms/step - loss: 0.3238 - acc: 0.8879 - val_loss: 0.3889 - val_acc: 0.8816\n","\n","Epoch 00039: val_loss did not improve from 0.35683\n","Epoch 40/55\n","703/703 [==============================] - 183s 261ms/step - loss: 0.3159 - acc: 0.8896 - val_loss: 0.4409 - val_acc: 0.8748\n","\n","Epoch 00040: val_loss did not improve from 0.35683\n","Epoch 41/55\n","703/703 [==============================] - 183s 261ms/step - loss: 0.3219 - acc: 0.8881 - val_loss: 0.4337 - val_acc: 0.8760\n","\n","Epoch 00041: val_loss did not improve from 0.35683\n","Epoch 42/55\n","703/703 [==============================] - 183s 261ms/step - loss: 0.3112 - acc: 0.8915 - val_loss: 0.5266 - val_acc: 0.8532\n","\n","Epoch 00042: val_loss did not improve from 0.35683\n","Epoch 43/55\n","703/703 [==============================] - 183s 260ms/step - loss: 0.3017 - acc: 0.8940 - val_loss: 0.5155 - val_acc: 0.8490\n","\n","Epoch 00043: val_loss did not improve from 0.35683\n","Epoch 44/55\n","703/703 [==============================] - 183s 260ms/step - loss: 0.3038 - acc: 0.8934 - val_loss: 0.4008 - val_acc: 0.8784\n","\n","Epoch 00044: val_loss did not improve from 0.35683\n","Epoch 45/55\n","703/703 [==============================] - 183s 260ms/step - loss: 0.2988 - acc: 0.8949 - val_loss: 0.4185 - val_acc: 0.8724\n","\n","Epoch 00045: val_loss did not improve from 0.35683\n","Epoch 46/55\n","703/703 [==============================] - 183s 260ms/step - loss: 0.2987 - acc: 0.8947 - val_loss: 0.4154 - val_acc: 0.8812\n","\n","Epoch 00046: val_loss did not improve from 0.35683\n","Epoch 47/55\n","703/703 [==============================] - 183s 261ms/step - loss: 0.2915 - acc: 0.8975 - val_loss: 0.5400 - val_acc: 0.8516\n","\n","Epoch 00047: val_loss did not improve from 0.35683\n","Epoch 48/55\n","703/703 [==============================] - 183s 261ms/step - loss: 0.2839 - acc: 0.9006 - val_loss: 0.5576 - val_acc: 0.8482\n","\n","Epoch 00048: val_loss did not improve from 0.35683\n","Epoch 49/55\n","703/703 [==============================] - 183s 261ms/step - loss: 0.2803 - acc: 0.9027 - val_loss: 0.5135 - val_acc: 0.8622\n","\n","Epoch 00049: val_loss did not improve from 0.35683\n","Epoch 50/55\n","703/703 [==============================] - 183s 260ms/step - loss: 0.2787 - acc: 0.9029 - val_loss: 0.3556 - val_acc: 0.8930\n","\n","Epoch 00050: val_loss improved from 0.35683 to 0.35559, saving model to 050-0.893.hdf5\n","Epoch 51/55\n","703/703 [==============================] - 183s 260ms/step - loss: 0.2386 - acc: 0.9167 - val_loss: 0.3082 - val_acc: 0.9108\n","\n","Epoch 00051: val_loss improved from 0.35559 to 0.30817, saving model to 051-0.911.hdf5\n","Epoch 52/55\n","703/703 [==============================] - 183s 260ms/step - loss: 0.2234 - acc: 0.9223 - val_loss: 0.3033 - val_acc: 0.9092\n","\n","Epoch 00052: val_loss improved from 0.30817 to 0.30331, saving model to 052-0.909.hdf5\n","Epoch 53/55\n","703/703 [==============================] - 183s 261ms/step - loss: 0.2169 - acc: 0.9237 - val_loss: 0.2967 - val_acc: 0.9140\n","\n","Epoch 00053: val_loss improved from 0.30331 to 0.29666, saving model to 053-0.914.hdf5\n","Epoch 54/55\n","703/703 [==============================] - 183s 261ms/step - loss: 0.2137 - acc: 0.9256 - val_loss: 0.2987 - val_acc: 0.9098\n","\n","Epoch 00054: val_loss did not improve from 0.29666\n","Epoch 55/55\n","703/703 [==============================] - 183s 261ms/step - loss: 0.2131 - acc: 0.9253 - val_loss: 0.2907 - val_acc: 0.9108\n","\n","Epoch 00055: val_loss improved from 0.29666 to 0.29070, saving model to 055-0.911.hdf5\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f5169637710>"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"8dbf4392-91e9-42b3-a8fb-6e01d0798ed0","executionInfo":{"status":"ok","timestamp":1567682720799,"user_tz":-330,"elapsed":467564,"user":{"displayName":"Rishav Putatunda","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB8kwyOxYTSXj0jnrae5i0B54hb9hMZyLZRV1Ek=s64","userId":"02963166143804289584"}},"id":"zQwamQrjCpQi","colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["# fits the model on batches with real-time data augmentation:\n","model.fit_generator(datagen_train.flow(X_train, y_train, batch_size=128),\n","                    steps_per_epoch=len(X_train)//128, epochs=65, initial_epoch=62, verbose=1, validation_data=(X_cv, y_cv), callbacks=call)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 63/65\n","351/351 [==============================] - 155s 442ms/step - loss: 0.1968 - acc: 0.9300 - val_loss: 0.2938 - val_acc: 0.9110\n","\n","Epoch 00063: val_loss did not improve from 0.28450\n","Epoch 64/65\n","351/351 [==============================] - 155s 442ms/step - loss: 0.1929 - acc: 0.9327 - val_loss: 0.2745 - val_acc: 0.9162\n","\n","Epoch 00064: val_loss improved from 0.28450 to 0.27454, saving model to 064-0.916.hdf5\n","Epoch 65/65\n","351/351 [==============================] - 155s 442ms/step - loss: 0.1894 - acc: 0.9330 - val_loss: 0.2910 - val_acc: 0.9134\n","\n","Epoch 00065: val_loss did not improve from 0.27454\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f51691f0d68>"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"92ba2287-2b69-4001-a89a-f5bf682ae6ae","executionInfo":{"status":"ok","timestamp":1567682739629,"user_tz":-330,"elapsed":11886,"user":{"displayName":"Rishav Putatunda","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB8kwyOxYTSXj0jnrae5i0B54hb9hMZyLZRV1Ek=s64","userId":"02963166143804289584"}},"id":"xzib4_rSCibF","colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["score = model.evaluate(X_test, y_test, verbose=1)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["10000/10000 [==============================] - 11s 1ms/step\n","Test loss: 0.3304223409116268\n","Test accuracy: 0.9038\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UE3lF6EH1r_L","outputId":"4c7fe8c4-5ef0-47a8-b5ba-37db90dae020","executionInfo":{"status":"ok","timestamp":1567542545709,"user_tz":-330,"elapsed":1589,"user":{"displayName":"Rishav Putatunda","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB8kwyOxYTSXj0jnrae5i0B54hb9hMZyLZRV1Ek=s64","userId":"02963166143804289584"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Save the trained weights in to .h5 format\n","model.save(\"DNST_model2.h5\")\n","print(\"Saved model to disk\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Saved model to disk\n"],"name":"stdout"}]}]}